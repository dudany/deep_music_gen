# -*- coding: utf-8 -*-
"""RNN music Gen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JZ2rdud6KeaOTq25fTsCW32IfenCQJZ6
"""

!pip install pydub
!git clone https://github.com/dudany/deep_music_gen.git
import numpy as np
import pandas as pd
import pydub
import os
from tqdm import tqdm
from collections import namedtuple
from keras.layers import Dense, LSTM, LeakyReLU
from keras.models import Sequential, load_model
from scipy.io.wavfile import read, write

"""**If you want to train your model on mp3 files, the following lines of code will do the trick.**"""

# converting mp3 file to wav file
# sound = pydub.AudioSegment.from_mp3("Numb_piano.mp3")
# sound.export("Numb.wav", format="wav")
# sound = pydub.AudioSegment.from_mp3("Eminem.mp3")
# sound.export("Eminem.wav", format="wav")

"""as we Have .wav data set we'll not to the conversion"""

# loading the wav files
MusicTup = namedtuple('MusicTup', ['rate', 'music'])
data_path = '/content/deep_music_gen/Full_Drum_Loops'
data_list = []


for i,f in enumerate(os.listdir(data_path)): #todo Remove the i filter for full data
  if i<1:
    path2f = os.path.join(data_path, f)
    if os.path.isfile(path2f):
      rate, music = read(path2f)
      music = pd.DataFrame(music)
      data_list.append(MusicTup(rate,music))

"""We can clearly see that the music dataframe has two columns, namely the two channels of our song and it is of integer type. What we are going to do is train two models on the two channels and make predictions separately for both the channels and then concatenate the results to generate a piece of music."""

# function to create training data by shifting the music data 
def create_train_dataset(df, look_back, train=True):
    dataX1, dataX2 , dataY1 , dataY2 = [],[],[],[]
    for i in tqdm(range(len(df)-look_back-1)):
        dataX1.append(df.iloc[i : i + look_back, 0].values)
        dataX2.append(df.iloc[i : i + look_back, 1].values)
        if train:
            dataY1.append(df.iloc[i + look_back, 0])
            dataY2.append(df.iloc[i + look_back, 1])
    if train:
        return np.array(dataX1), np.array(dataX2), np.array(dataY1), np.array(dataY2)
    else:
        return np.array(dataX1), np.array(dataX2)
x_list = []
y_list = []
df = pd.DataFrame()
for data_tup in data_list:
  df = pd.concat([df, data_tup.music])

train_amount = int(len(df) * (8/10))
X1, X2, y1, y2 = create_train_dataset(df[:train_amount], look_back=3, train=True)
test1, test2 = create_train_dataset(df[train_amount+1:],look_back=3,train=False)

X1 = X1.reshape((-1, 1, 3))
X2 = X2.reshape((-1, 1, 3))
test1 = test1.reshape((-1, 1, 3))
test2 = test2.reshape((-1, 1, 3))

# LSTM Model for channel 1 of the music data
class MusicGenrModel(object):
  def __init__(self,x,y,epochs=20,batch_size=100):
    self.model = Sequential()
    self.model.add(LSTM(units=100, activation='relu', input_shape=(None, 3)))
    self.model.add(Dense(units=50, activation='relu'))
    self.model.add(Dense(units=25, activation='relu'))
    self.model.add(Dense(units=12, activation='relu'))
    self.model.add(Dense(units=1, activation='relu'))
    self.model.compile(optimizer='adam', loss='mean_squared_error')
    self.model.fit(x,y,epochs,batch_size)
    


rnn1 = MusicGenrModel(X1, y1)

